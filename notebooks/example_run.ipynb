{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.x",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lightweight AutoML Demo \u2014 Iris Dataset\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "\n",
        "1. Run the **Lightweight AutoML** system on the Iris dataset.\n",
        "2. Inspect the resulting leaderboard of models.\n",
        "3. Load the saved **best model checkpoint** for Iris.\n",
        "4. Evaluate the model and visualize performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Make sure you've installed the project requirements (from the repo root):\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "Then run this notebook **from the project root directory**, so paths like `data/iris.csv` and `automl_orchestrator.py` resolve correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Imports and path setup\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Ensure we can import from the project root\n",
        "PROJECT_ROOT = Path(os.getcwd()).resolve()\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "\n",
        "import sys\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "from automl_orchestrator import run_automl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Run AutoML on Iris\n",
        "\n",
        "This will:\n",
        "- Load `data/iris.csv`\n",
        "- Automatically infer schema and build preprocessing\n",
        "- Run hyperparameter search over the model zoo\n",
        "- Build ensembles\n",
        "- Save a leaderboard CSV in `results/`\n",
        "- Save the **best overall model** checkpoint to `checkpoints/best_model_iris.pkl`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "iris_path = \"data/iris.csv\"\n",
        "target_col = \"class\"\n",
        "\n",
        "leaderboard_df = run_automl(data_path=iris_path, target_column=target_col)\n",
        "\n",
        "leaderboard_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load the Best Iris Model Checkpoint\n",
        "\n",
        "The orchestrator saves the best model (by held-out test score) for each dataset as:\n",
        "\n",
        "```text\n",
        "checkpoints/best_model_<dataset_name>.pkl\n",
        "```\n",
        "\n",
        "For Iris, this should be `checkpoints/best_model_iris.pkl`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import joblib\n",
        "\n",
        "checkpoint_path = Path(\"checkpoints\") / \"best_model_iris.pkl\"\n",
        "print(\"Checkpoint path:\", checkpoint_path)\n",
        "\n",
        "best_model_iris = joblib.load(checkpoint_path)\n",
        "best_model_iris\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate the Best Model on the Full Iris Dataset\n",
        "\n",
        "Here we:\n",
        "- Reload `iris.csv`\n",
        "- Separate features and target\n",
        "- Use the saved best model to predict\n",
        "- Print a classification report and confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "iris_df = pd.read_csv(iris_path)\n",
        "X_iris = iris_df.drop(columns=[target_col])\n",
        "y_iris = iris_df[target_col]\n",
        "\n",
        "y_pred = best_model_iris.predict(X_iris)\n",
        "\n",
        "print(\"Classification report (best Iris model):\\n\")\n",
        "print(classification_report(y_iris, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_iris, y_pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix Heatmap\n",
        "\n",
        "Let's visualize the confusion matrix for a clearer view of which classes are confused."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(4, 4))\n",
        "im = ax.imshow(cm, interpolation=\"nearest\")\n",
        "ax.set_title(\"Iris \u2014 Confusion Matrix (Best Model)\")\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "classes = sorted(y_iris.unique())\n",
        "ax.set_xticks(range(len(classes)))\n",
        "ax.set_yticks(range(len(classes)))\n",
        "ax.set_xticklabels(classes, rotation=45, ha=\"right\")\n",
        "ax.set_yticklabels(classes)\n",
        "ax.set_xlabel(\"Predicted label\")\n",
        "ax.set_ylabel(\"True label\")\n",
        "\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize Leaderboard Scores\n",
        "\n",
        "Finally, let's make a simple bar chart of the test scores for each model on Iris. This corresponds directly to the model zoo performance described in the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(leaderboard_df[\"model\"], leaderboard_df[\"test_score\"])\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Test Score\")\n",
        "plt.title(\"Model Zoo Performance on Iris (Test Scores)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "\n",
        "In this notebook, we:\n",
        "- Ran the **Lightweight AutoML** system on the Iris dataset.\n",
        "- Inspected the model leaderboard.\n",
        "- Loaded the **best model checkpoint** and evaluated it on the full dataset.\n",
        "- Visualized the confusion matrix and model zoo performance.\n",
        "\n",
        "This example demonstrates the full pipeline from **AutoML orchestration** to **saved checkpoints** and **downstream analysis**, matching the behavior described in the final report."
      ]
    }
  ]
}